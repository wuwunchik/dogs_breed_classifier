import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image
import wikipedia
from breed_translation import breed_translation

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    return tf.keras.models.load_model("dogs_model.keras")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –ø–æ—Ä–æ–¥
@st.cache_data
def load_class_names():
    with open("class_names.txt", "r") as f:
        return [line.strip() for line in f.readlines()]

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
def preprocess_image(image, target_size=(224, 224)):
    image = image.resize(target_size)
    img_array = tf.keras.preprocessing.image.img_to_array(image)
    img_array = tf.expand_dims(img_array, axis=0)
    return tf.keras.applications.efficientnet.preprocess_input(img_array)

def get_wiki_description_and_link(breed_name):
    try:
        breed_ru = breed_translation.get(breed_name, breed_name)
        wikipedia.set_lang('ru')  
        search_results = wikipedia.search(breed_ru, results=1)
        
        if search_results:
            page_title = search_results[0]
            wiki_page = wikipedia.page(page_title)
            return wiki_page.summary[:500] + "...", wiki_page.url  
        else:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Ä–æ–¥–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ –í–∏–∫–∏–ø–µ–¥–∏–∏.", None
    except wikipedia.exceptions.DisambiguationError as e:
        return f"–ï—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è {breed_name}. –ù–∞–ø—Ä–∏–º–µ—Ä: {', '.join(e.options[:5])}.", None
    except wikipedia.exceptions.RedirectError as e:
        return f"–ü—Ä–æ–∏–∑–æ—à–µ–ª —Ä–µ–¥–∏—Ä–µ–∫—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ {breed_name}.", None
    except wikipedia.exceptions.HTTPTimeoutError as e:
        return "–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –í–∏–∫–∏–ø–µ–¥–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.", None
    except Exception as e:
        return str(e), None

st.title("üê∂ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–¥—ã —Å–æ–±–∞–∫–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é")

uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–±–∞–∫–∏", type=["jpg", "jpeg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_container_width=True)

    model = load_model()
    class_names = load_class_names()

    img_array = preprocess_image(image)
    prediction = model.predict(img_array)[0]
    predicted_idx = np.argmax(prediction)
    confidence = prediction[predicted_idx]
    predicted_breed = class_names[predicted_idx]

    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Ä–æ–¥—ã –∏ —Å—Å—ã–ª–∫—É –Ω–∞ –í–∏–∫–∏–ø–µ–¥–∏—é
    breed_description, wiki_url = get_wiki_description_and_link(predicted_breed)

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    st.markdown(f"### üêï –ü–æ—Ä–æ–¥–∞: **{breed_translation.get(predicted_breed, predicted_breed)}**")
    st.markdown(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: **{confidence:.2%}**")
    st.markdown(f"**–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Ä–æ–¥—ã**: {breed_description}")

    if wiki_url:
        st.markdown(f"[–ü–æ–¥—Ä–æ–±–Ω–µ–µ –Ω–∞ –í–∏–∫–∏–ø–µ–¥–∏–∏]({wiki_url})")
